---
title: "Projet de MRR"
author: "Elena BOUCHAUD, Hanna BEN JEDIDIA et Aymeric MARBOEUF"
date: "7 janvier 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Nous disposons d'un jeu de données comprenant des informations sur le poids et des paramètres susceptibles d'aider à déterminer le poids d'une personne tels que la taille, le tour des hanches... À l'aide de ce jeu de données, nous souhaitons trouver un modèle qui détermine le poids d'une personne en fonction des différentes variables.

À l'origine, ce jeu de données est composé de 252 observations et de 19 variables: ID de la personne, le pourcentage de graisse corporelle selon l'équation de Brozek, le pourcentage de graisse corporelle selon l'équation de Siri, la densité, l'âge, le poids, la taille, l'indice d'adiposité, le poids sans gras, la circonférence du cou, la circonférence de la poitrine, la circonférence de l'abdomen, la circonférence des hanches, la circonférence des cuisses, la circonférence des genoux, la circonférence des chevilles, la circonférence du biceps, la circonférence de l'avant bras et la circonférence du poignet.

```{r echo = FALSE, message = FALSE}
library(MASS)
library(corrplot)
library(glmnet)
library(hydroGOF)
library(class)

tab = data.frame(read.table("FA.dat", sep = "", header = TRUE))
boxplot(tab)
```

## Analyse et nettoyage des données

La première colonne, l'ID de la personne, ne donne pas d'informations sur le poids elle est présente à titre indicatif. Il est donc préférable de supprimer cette colonne pour avoir une bonne modélisation.
Les cas 48, 76 et 96 ont des erreurs dans les données de pourcentage de gras. Nous allons donc supprimer ces lignes pour ne pas corrompre nos données. Le cas 182 a un pourcentage de gras négatif ce qui n'est pas possible, nous supprimons cette ligne également. D'après l'analyse des données la taille du cas 42 est éronnée mais peut être corrigée.

```{r echo = FALSE}
tab = tab[-c(48, 76, 96,182),-1]
tab$Height[42] = 69.5
```

En analysant un peu plus les données, nous remarquons qu'il y a deux valeurs parmi celles du poids qui semblent aberrantes ou qui apartiennent à des cas de grandes obésités et qui peut corrompre nos modèles. Une valeur aussi parmi celle du poids sans gras semble aberrante (108kg) ainsi que deux autres parmi la circonférence de la poitrine. Les lignes contenant ces valeurs seront aussi enlevées.

```{r echo = FALSE}
m = which(grepl(max(tab$Weight),tab$Weight))
tab = tab[-c(m),]
m = which(grepl(max(tab$Weight),tab$Weight))
tab = tab[-c(m),]
m = which(grepl(max(tab$Fat_Free_Weight),tab$Fat_Free_Weight))
tab = tab[-c(m),]
m = which(grepl(max(tab$Chest_circumference),tab$Chest_circumference))
tab = tab[-c(m),]
m = which(grepl(max(tab$Chest_circumference),tab$Chest_circumference))
tab = tab[-c(m),]
boxplot(tab)
```

Nous allons donc travailler avec un jeu de données contenant 243 observations et 18 variables.

## Corrélations entre les données

```{r echo = FALSE}
corrplot(cor(tab),tl.cex=0.5)
```
Nous observons que les variables sont fortement corélées entre elles. Or il n'est pas préférable d'utiliser une régression linéaire. Nous allons donc effectuer une régression par pénalisation.

```{r echo = FALSE}
n = dim(tab)[1]
tabSim = tab[setdiff(1:n,0:floor(n/5)),] 
tabTest = tab[0:floor(n/5),]
tabTest2 = tabTest[-5]
tabSim2= tabSim[-5]
Sim = as.matrix(data.frame(tabSim2))
```

## Setpwise, Forward et Backward

Nous pouvons utiliser des méthodes de sélections de variables tel que Stepwise, forward et backward.

```{r echo = FALSE}
lmModel = lm(Weight~., data = tab)
stepModel = stepAIC(lmModel, direction = "both", trace = FALSE)
forwardModel = stepAIC(lmModel, direction = "forward", trace = FALSE)
backwardModel = stepAIC(lmModel, direction = "backward", trace = FALSE)
```

Le adjusted R-squared de ces méthodes est  \({R^2}_{adj} = 0.9931\). Le adjusted R-squared est proche de 1, donc ces méthodes sont efficaces pour la sélection de variables.

## Ridge

Nous allons effectuer une régression l2 pénalisée avec Ridge. Nous choisissons le \(\lambda\) par cross validation.

```{r echo = FALSE, warning= FALSE}
modridge2 = glmnet(Sim, tabSim$Weight, alpha=0, family="gaussian")
cvModridge2= cv.glmnet(Sim,y= tabSim$Weight, alpha=0)
bestLambdaRidge2 = cvModridge2$lambda.min
predictRidge = predict(modridge2, as.matrix(tabTest2),s=bestLambdaRidge2,type="class")
rmse(as.numeric(tabTest$Weight),as.numeric(predictRidge))
rmseRidge = sqrt(mean((tabTest$Weight-predictRidge)^2))
```

Nous avons un RMSE de 3.330613.

## Lasso

Nous allons effectuer une régression l1 pénalisée avec Lasso. Nous choisissons le \(\lambda\) par cross validation.

```{r echo = FALSE}
modlasso2 = glmnet(Sim, tabSim$Weight, alpha=1, family="gaussian")
cvModlasso2= cv.glmnet(Sim,y= tabSim$Weight, alpha=1)
bestLambdaLasso2 = cvModlasso2$lambda.min
predictLass = predict(modlasso2, as.matrix(tabTest2),s=bestLambdaLasso2,type="class")
rmseLasso = sqrt(mean((tabTest$Weight-predictLass)^2))
```
Nous avons un RMSE de 2.344981. Le RMSE de la méthode Lasso est plus petit que celui de la méthode Ridge. Lasso modélise mieux nos données que Ridge. 

## Elastic Net

Elastic Net est une méthode de régression régularisé qui combine Lasso et Ridge. Nous choississons le \(\alpha\) qui minimise le RMSE.

```{r echo = FALSE}
matRMSE = vector(length = 9)
for (i in 1:9){
  al = i/10
 modEN = glmnet(Sim, tabSim$Weight, alpha=al, family="gaussian")
  cvModEN= cv.glmnet(Sim,y= tabSim$Weight, alpha=al)
  bestLambdaEN = cvModEN$lambda.min
  predictEN = predict(modEN, as.matrix(tabTest2),s=bestLambdaEN,type="class")
  rmseEN = sqrt(mean((tabTest$Weight-predictEN)^2))
  matRMSE[i] = rmseEN
}
m = which(grepl(min(matRMSE), matRMSE))
for (i in 1:9){
  al = i/100
  al = al + (m/10)
  modEN = glmnet(Sim, tabSim$Weight, alpha=al, family="gaussian")
  cvModEN= cv.glmnet(Sim,y= tabSim$Weight, alpha=al)
  bestLambdaEN = cvModEN$lambda.min
  predictEN = predict(modEN, as.matrix(tabTest2),s=bestLambdaEN,type="class")
  rmseEN = sqrt(mean((tabTest$Weight-predictEN)^2))
  matRMSE[i] = rmseEN
}
almin = (m/10) + (which(grepl(min(matRMSE), matRMSE))/100)
min(matRMSE)
```
Nous avons un RMSE de 2.344349. Le RMSE de la méthode Elastic Net est assez semblable à celui de Lasso. Cette méthode permet aussi d'avoir une modélisation correcte.

## KNN
Nous allons utiliser une méthode d'apprentissage supervisé utilisé en machine learning. Celle-ci consiste à faire la moyenne des plus proches voisins d'une valeur à déterminer. Nous cherchons à minimiser la valeur du rmse et comparer cette valeur avec les autres modèles. On calcule donc les valeurs du rmse pour différentes valeurs de k afin de trouver le plus petit.

```{r echo = FALSE, warning= FALSE}
r = vector(length=500)
for(i in 1:500){
  knnModel = knn(train = tabSim2, test = tabTest2, cl = tabSim$Weight, k = i)
  m = tabSim$Weight-as.numeric(as.character(knnModel))
  rmseknn = sqrt(mean(m^2))
  r[i]=rmseknn
}
plot(r, xlab= "Valeur de k", ylab = "Valeur du RMSE", main = "Courbe du RMSE en fonction de k sans sélection de variables")
min(r)
which(grepl(min(r),r))
```

On observe que le k minimisant le RMSE se situe aux alentours de 100. Il faut donc prendre environ 100 voisins pour se rapprocher au mieux des valeurs réelles. Cependant, on obtient un  RMSE de 24 environ ce qui est une valeur très importante par rapport aux valeurs de la RMSE trouvés avec les autres modèles.
